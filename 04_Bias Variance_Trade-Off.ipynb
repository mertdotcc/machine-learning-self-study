{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Bias Variance Trade-Off__ is a fundamental topic of understanding our model's performance.\n",
    "<br><br>\n",
    "* Use the Chapter 2 of the book __ISLR__ for more in depth look.\n",
    "<br><br>\n",
    "* The bias-variance trade-off is the point where we are adding noise by adding model complexity (flexibility).\n",
    "<br><br>\n",
    "* The training error goes down as it has to, but the test error is starting to go up.\n",
    "<br><br>\n",
    "* The model after the bias trade-off begins to overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "* Imagine that the center of the target is a model that perfectly predicts the correct values.\n",
    "<br><br>\n",
    "* As we move away from the bulls-eye, our predictions get worse and worse.\n",
    "<br><br>\n",
    "![title](v1.png)\n",
    "<br><br>\n",
    "* Imagine we can repeat out entire model building process to get a number of separate hits on the target.\n",
    "<br><br>\n",
    "* Each hit represents an individual realization of our model, given the chance variability in the training data that we gather.\n",
    "<br><br>\n",
    "* Sometimes we will get a good distribution of training data so we predict very well and we are close to the bulls-eye, while sometimes our training data might be full of outliers or non-standard values resulting in poorer predictions.\n",
    "<br><br>\n",
    "* These different realizations result in a scatter of hits on the target.\n",
    "<br><br>\n",
    "* There is a temptation to continually add complexity to a model until it fits the training set very well.\n",
    "<br><br>\n",
    "![title](v2.png)\n",
    "<br><br>\n",
    "* Doing this can cause a model to overfit our training data and cause large errors on new data, such as the test set.\n",
    "<br><br>\n",
    "![title](v3.png)\n",
    "<br><br>\n",
    "* In the figure above, as we move to the left, we get a higher bias but a lower variance. \n",
    "<br><br>\n",
    "* And as we move to the right, to a higher complexity model, we get a lower bias but higher variance.\n",
    "<br><br>\n",
    "* And what we want to do is to pick a point where we are comfortable with the bias trade-off, cause if we go to the left of it, we start to underfit the data, and vice versa, if we go to the right of it, we start to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of the notebook. Refer to the chapter 2 of the book ISLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
